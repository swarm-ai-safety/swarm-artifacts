---
description: Fast-adapting RLM agents outperform honest agents by 38% but via superior partner selection, not governance evasion
type: claim
status: active
confidence: medium
domain: agent-behavior
evidence:
  supporting:
  - run: 20260210-215826_analysis_rlm_governance_lag
    metric: payoff
    detail: "RLM fast (d=5, horizon=8) mean 335.61 vs honest 242.76, d=2.14, p=0.00015, Bonferroni-sig. 10 seeds, 10 agents per seed. RLM signal profiles produce p~0.5-0.7, well above audit threshold (p<0.4), so governance rarely triggers"
  weakening: []
  boundary_conditions:
  - "Small-world topology (k=4, p=0.15) with audit_probability=0.05"
  - "Adaptive adversary bimodal: either dominates (~665-939) or frozen (~0-5), std=174.55"
  - "Algorithmic level-k reasoners, not LLM-powered agents"
  - "ANOVA marginal (p=0.055) due to adversary variance — pairwise RLM-vs-honest drives signal"
  - "10 seeds with fixed a priori selection"
sensitivity:
  topology: untested beyond small_world
  agent_count: 10 agents (4 RLM, 4 honest, 2 adversary)
  governance_strength: tested with low audit probability (0.05) only
supersedes: []
superseded_by: []
related_claims:
- claim-smarter-agents-earn-less
- claim-circuit-breakers-dominate
- claim-agent-architecture-constrains-behavior-more-than-governance
created: 2026-02-27
updated: 2026-02-27
aliases:
- rlm-governance-lag
cssclasses:
- claim
- claim-high
tags:
- agent-behavior
- rlm
- governance-lag
- strategic-reasoning
graph-group: claim
---

# RLM agents exploit the governance lag not by evading detection but by using strategic depth for superior partner selection

## Evidence summary

The governance lag hypothesis predicted that fast-adapting RLM agents (recursion depth 5, planning horizon 8) would outpace governance response times and exploit the detection gap. The [[20260210-215826_analysis_rlm_governance_lag]] analysis (10 seeds, 10 agents) confirms the payoff advantage but reveals a different mechanism:

| Group | Mean Payoff | Std | n (per seed) |
|-------|------------|-----|--------------|
| RLM fast (d=5) | 335.61 | 19.60 | 4 |
| Honest | 242.76 | 58.13 | 4 |
| Adaptive adversary | 221.49 | 174.55 | 2 |

RLM vs honest: t=4.79, p=0.00015, Cohen's d=2.14 (Bonferroni-significant).

## Mechanism

RLM agents do NOT evade governance. Their signal profiles produce moderate p-values (~0.5-0.7), well above the harm threshold (p < 0.4), so audits and circuit breakers rarely trigger. Instead, their strategic advantage comes from:

1. **Superior partner selection** within the small-world network
2. **Better cooperation timing** enabled by 8-step planning horizons
3. **Consistent performance** (std=19.60 vs honest std=58.13)

This is governance lag through strategic depth, not governance evasion — the gap between what governance monitors and what actually drives payoff.

## Adaptive adversary bimodality

Adversary payoffs are bimodal (std=174.55): some seeds produce dominance (~469-939) while others result in complete shutdown (~0-5). This boom-or-bust pattern masks the RLM-vs-adversary comparison (p=0.055, non-significant).

## Gini coefficient

Gini = 0.3253 (significant, p < 0.001), driven by adversary bimodality and honest variance from network position effects.

## Implications

1. Governance mechanisms that only monitor harm signals miss strategic advantages from reasoning depth
2. RLM advantage is robust (consistent across seeds) while adversary advantage is fragile (seed-dependent)
3. The governance gap is in what gets measured, not in detection speed

---

Topics:
- [[_index]]
- [[governance-dashboard]]

<!-- topics: agent-behavior, rlm, governance-lag, strategic-reasoning -->
